\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{apa/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{arlot2010survey}
\abx@aux@segm{0}{0}{arlot2010survey}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{BurmanChowNolan1994}
\abx@aux@segm{0}{0}{BurmanChowNolan1994}
\abx@aux@cite{0}{Racine2000}
\abx@aux@segm{0}{0}{Racine2000}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wood2024neighbourhood}
\abx@aux@segm{0}{0}{wood2024neighbourhood}
\babel@aux{american}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\abx@aux@page{1}{1}
\abx@aux@page{2}{1}
\abx@aux@page{3}{1}
\@writefile{toc}{\contentsline {subsection}{Research Question}{1}{section*.1}\protected@file@percent }
\abx@aux@page{4}{1}
\@writefile{toc}{\contentsline {subsection}{Outline}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Framework}{2}{section.2}\protected@file@percent }
\newlabel{eq:cv_generic}{{2}{2}{Theoretical Framework}{equation.2}{}}
\@writefile{toc}{\contentsline {subsection}{Covariance Structure and Autocorrelation}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{The Failure of Naive Random $k$-fold Cross-Validation}{2}{section*.4}\protected@file@percent }
\newlabel{sec:naive_failure}{{2}{2}{The Failure of Naive Random $k$-fold Cross-Validation}{section*.4}{}}
\newlabel{eq:cv_random_kfold}{{4}{2}{The Failure of Naive Random $k$-fold Cross-Validation}{equation.4}{}}
\newlabel{claim:bias}{{1}{2}{Bias of Random $k$-fold CV}{theorem.1}{}}
\@writefile{toc}{\contentsline {subsection}{Dependence-Aware CV Schemes.}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Block $K$-fold CV.}{3}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual comparison of Cross-Validation schemes. From top left to bottom right: Naive Random $K$-fold, Block CV, Buffered CV and Walk-Forward Validation. Leave $2\ell +1$ CV is the same as Buffered CV with one datapoint in the validation set. Red indicates validation sets, blue training sets, and gray the excluded buffer zones.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:cv_schemes}{{1}{3}{Visual comparison of Cross-Validation schemes. From top left to bottom right: Naive Random $K$-fold, Block CV, Buffered CV and Walk-Forward Validation. Leave $2\ell +1$ CV is the same as Buffered CV with one datapoint in the validation set. Red indicates validation sets, blue training sets, and gray the excluded buffer zones}{figure.1}{}}
\@writefile{toc}{\contentsline {subparagraph}{Buffered Block CV.}{3}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Leave-$(2\ell +1)$-out Neighborhood CV (NCV).}{3}{section*.8}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{Nadaraya1964}
\abx@aux@segm{0}{0}{Nadaraya1964}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{EilersMarx1996}
\abx@aux@segm{0}{0}{EilersMarx1996}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{ChenGuestrin2016}
\abx@aux@segm{0}{0}{ChenGuestrin2016}
\@writefile{toc}{\contentsline {subparagraph}{Walk-forward (Rolling-Origin) Validation.}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Regression Models and Hyperparameters}{4}{section.3}\protected@file@percent }
\newlabel{sec:models}{{3}{4}{Regression Models and Hyperparameters}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{Kernel Regression (Nadaraya--Watson)}{4}{section*.10}\protected@file@percent }
\abx@aux@page{5}{4}
\newlabel{eq:nw}{{5}{4}{Kernel Regression (Nadaraya--Watson)}{equation.5}{}}
\@writefile{toc}{\contentsline {subsection}{Penalized Splines}{4}{section*.11}\protected@file@percent }
\abx@aux@page{6}{4}
\newlabel{eq:pspline_obj}{{6}{4}{Penalized Splines}{equation.6}{}}
\newlabel{eq:pspline_closed}{{7}{4}{Penalized Splines}{equation.7}{}}
\@writefile{toc}{\contentsline {subsection}{Gradient-boosted Trees (XGBoost)}{4}{section*.12}\protected@file@percent }
\abx@aux@page{7}{4}
\newlabel{eq:xgb_additive}{{8}{4}{Gradient-boosted Trees (XGBoost)}{equation.8}{}}
\@writefile{toc}{\contentsline {subsection}{Summary of Tuned Hyperparameters}{4}{section*.13}\protected@file@percent }
\newlabel{sec:hyper_summary}{{3}{4}{Summary of Tuned Hyperparameters}{section*.13}{}}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wood2024neighbourhood}
\abx@aux@segm{0}{0}{wood2024neighbourhood}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wood2024neighbourhood}
\abx@aux@segm{0}{0}{wood2024neighbourhood}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of regression models and primary complexity hyperparameters tuned by cross-validation. Smaller $h$ and $\lambda $ correspond to less smoothing (higher variance), while larger tree depth $d$ increases model flexibility.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:comparison}{{1}{5}{Summary of regression models and primary complexity hyperparameters tuned by cross-validation. Smaller $h$ and $\lambda $ correspond to less smoothing (higher variance), while larger tree depth $d$ increases model flexibility}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Efficient Computation for Penalized Splines (NCV)}{5}{section.4}\protected@file@percent }
\newlabel{sec:ncv_computation}{{4}{5}{Efficient Computation for Penalized Splines (NCV)}{section.4}{}}
\abx@aux@page{8}{5}
\@writefile{toc}{\contentsline {subsection}{Penalized Spline Estimator}{5}{section*.14}\protected@file@percent }
\newlabel{sec:spline_estimator}{{4}{5}{Penalized Spline Estimator}{section*.14}{}}
\newlabel{eq:pen_obj}{{9}{5}{Penalized Spline Estimator}{equation.9}{}}
\@writefile{toc}{\contentsline {subsection}{Neighborhood CV Objective and Naive Cost}{5}{section*.15}\protected@file@percent }
\newlabel{sec:ncv_objective}{{4}{5}{Neighborhood CV Objective and Naive Cost}{section*.15}{}}
\newlabel{eq:ncv_def}{{10}{5}{Neighborhood CV Objective and Naive Cost}{equation.10}{}}
\newlabel{eq:ncv_def2}{{11}{5}{Neighborhood CV Objective and Naive Cost}{equation.11}{}}
\@writefile{toc}{\contentsline {subparagraph}{Naive Complexity.}{5}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{FastNCV idea: one Newton Step from the Full-data Solution}{5}{section*.17}\protected@file@percent }
\newlabel{sec:newton_update}{{4}{5}{FastNCV idea: one Newton Step from the Full-data Solution}{section*.17}{}}
\abx@aux@page{9}{5}
\newlabel{eq:newton_step}{{12}{5}{FastNCV idea: one Newton Step from the Full-data Solution}{equation.12}{}}
\@writefile{toc}{\contentsline {subparagraph}{Accuracy vs Exactness.}{5}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Validation of Numerical Equivalence. Top: The fitted spline functions for Standard CV (thick blue line) and FastNCV (thin red dashed line) perfectly overlap. Bottom: The Cross-Validation error curves are identical, selecting the same minimum.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:equivalence}{{2}{6}{Validation of Numerical Equivalence. Top: The fitted spline functions for Standard CV (thick blue line) and FastNCV (thin red dashed line) perfectly overlap. Bottom: The Cross-Validation error curves are identical, selecting the same minimum}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{Cholesky downdating and Woodbury fallback}{6}{section*.19}\protected@file@percent }
\newlabel{sec:chol_downdate}{{4}{6}{Cholesky downdating and Woodbury fallback}{section*.19}{}}
\@writefile{toc}{\contentsline {subsection}{Algorithm summary}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Complexity}{6}{section*.21}\protected@file@percent }
\newlabel{sec:fastncv_complexity}{{4}{6}{Complexity}{section*.21}{}}
\@writefile{toc}{\contentsline {subsection}{Benchmark: Speed and Numerical Agreement}{6}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implication for the Main Experiments.}{6}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Computational efficiency comparison. Top: Execution time (seconds) vs. Sample Size for Standard NCV (red) and FastNCV (green). Bottom: Speedup factor achieved by the optimized implementation.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:time_comparison}{{3}{7}{Computational efficiency comparison. Top: Execution time (seconds) vs. Sample Size for Standard NCV (red) and FastNCV (green). Bottom: Speedup factor achieved by the optimized implementation}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulation Study}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Data generating process (DGP)}{7}{section*.24}\protected@file@percent }
\newlabel{sec:sim_dgp}{{5}{7}{Data generating process (DGP)}{section*.24}{}}
\newlabel{eq:dgp}{{13}{7}{Data generating process (DGP)}{equation.13}{}}
\@writefile{toc}{\contentsline {subparagraph}{Error Processes.}{7}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Signals}{7}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Local Bump.}{7}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Smooth Trend.}{7}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Nonlinear Curvature.}{7}{section*.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Autocorrelation functions (ACFs) of the simulated \emph  {error processes} used in the DGP.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:ACF}{{4}{8}{Autocorrelation functions (ACFs) of the simulated \emph {error processes} used in the DGP}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Signal families used in the simulation study (shown with representative noise realizations).}}{8}{figure.5}\protected@file@percent }
\newlabel{fig:signals}{{5}{8}{Signal families used in the simulation study (shown with representative noise realizations)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estimated risk versus independent test risk on synthetic data. Each point corresponds to one Monte Carlo replication, evaluated at the hyperparameter $\hat  \theta $ selected by the given CV scheme. The diagonal line indicates unbiased estimation. Configuration: signal = non-linear, error process = AR(1), $n=1000$.}}{9}{figure.6}\protected@file@percent }
\newlabel{fig:cv_reliability}{{6}{9}{Estimated risk versus independent test risk on synthetic data. Each point corresponds to one Monte Carlo replication, evaluated at the hyperparameter $\hat \theta $ selected by the given CV scheme. The diagonal line indicates unbiased estimation. Configuration: signal = non-linear, error process = AR(1), $n=1000$}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{9}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{9}{Results}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{Risk Estimation Bias}{9}{section*.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Estimated risk versus independent test risk on synthetic data. Each point corresponds to one Monte Carlo replication, evaluated at the hyperparameter $\hat  \theta $ selected by the given CV scheme. The diagonal line indicates unbiased estimation. Configuration: signal = non-linear, error process = ARIMA($2,0,5$) (Top), ARIMA($2,0,20$) (Bottom), $n=1000$.}}{10}{figure.7}\protected@file@percent }
\newlabel{fig:delta_ACF}{{7}{10}{Estimated risk versus independent test risk on synthetic data. Each point corresponds to one Monte Carlo replication, evaluated at the hyperparameter $\hat \theta $ selected by the given CV scheme. The diagonal line indicates unbiased estimation. Configuration: signal = non-linear, error process = ARIMA($2,0,5$) (Top), ARIMA($2,0,20$) (Bottom), $n=1000$}{figure.7}{}}
\@writefile{toc}{\contentsline {subparagraph}{Risk Estimation Bias Across Time-dependence.}{10}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Hyperparameter Selection}{10}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Hyperparameter analysis for kernel regression. Top panel: distribution of the selected bandwidth $h$ across Monte Carlo replications for each cross-validation scheme (Naive, Block, Buffered, Walk-forward); dots indicate the mean and boxes show the interquartile range. Bottom panel: stability of selection, measured as the percent deviation of the selected $h$ from the scheme-specific median (lower magnitude indicates more stable selection). Configuration; signal = non-linear, error process = AR(1).}}{10}{figure.8}\protected@file@percent }
\newlabel{fig:hyperparameter_analysis}{{8}{10}{Hyperparameter analysis for kernel regression. Top panel: distribution of the selected bandwidth $h$ across Monte Carlo replications for each cross-validation scheme (Naive, Block, Buffered, Walk-forward); dots indicate the mean and boxes show the interquartile range. Bottom panel: stability of selection, measured as the percent deviation of the selected $h$ from the scheme-specific median (lower magnitude indicates more stable selection). Configuration; signal = non-linear, error process = AR(1)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{Comparison of Fitted Curves}{10}{section*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Qualitative comparison of fitted functions under different CV schemes. Configuration: error = MA(5), signal = non-linear.}}{11}{figure.9}\protected@file@percent }
\newlabel{fig:fit_synthetic}{{9}{11}{Qualitative comparison of fitted functions under different CV schemes. Configuration: error = MA(5), signal = non-linear}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of CV schemes performance under different regression methods and across 100 independent simulations. Configuration: error = MA(5), signal = non-linear.}}{11}{figure.10}\protected@file@percent }
\newlabel{fig:boxplots_synthetic}{{10}{11}{Comparison of CV schemes performance under different regression methods and across 100 independent simulations. Configuration: error = MA(5), signal = non-linear}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{Model Comparison}{12}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Time Comparison}{12}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average running time (seconds; log-scale color map) for cross-validation under the nonlinear signal. Rows correspond to the error type (Seasonal, ARIMA, MA(5), AR(1)); columns correspond to the regression method (Kernel, Spline, XGBoost). Within each panel, the four entries report the mean wall-clock time for Naive randomized K-fold, Block K-fold, Buffered block, and Walk-forward validation. Darker shading indicates longer runtime.}}{12}{figure.11}\protected@file@percent }
\newlabel{fig:heatmap}{{11}{12}{Average running time (seconds; log-scale color map) for cross-validation under the nonlinear signal. Rows correspond to the error type (Seasonal, ARIMA, MA(5), AR(1)); columns correspond to the regression method (Kernel, Spline, XGBoost). Within each panel, the four entries report the mean wall-clock time for Naive randomized K-fold, Block K-fold, Buffered block, and Walk-forward validation. Darker shading indicates longer runtime}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Real Data Applications}{12}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Dataset Diagnostics and Characterization}{13}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Air Quality (Italy).}{13}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{CNNpred (S\&P 500).}{13}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Results}{13}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Air Quality (Italy).}{13}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{CNNpred (S\&P 500).}{13}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{13}{section.8}\protected@file@percent }
\newlabel{sec:discussion}{{8}{13}{Discussion}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{Key Takeaways}{13}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Air Quality} dataset: train (blue) and test-set predictions (red) under hyperparameters selected by each CV scheme. Rows correspond to regression methods (kernel regression, penalized splines, XGBoost) and columns to CV schemes (naive, block, buffered block, walk-forward). The dashed vertical line marks the train/test split; grey points denote the observed test values. Panel titles report the selected hyperparameter and the resulting test MSE.}}{14}{figure.12}\protected@file@percent }
\newlabel{fig:air_quality_train_test_preds}{{12}{14}{\textbf {Air Quality} dataset: train (blue) and test-set predictions (red) under hyperparameters selected by each CV scheme. Rows correspond to regression methods (kernel regression, penalized splines, XGBoost) and columns to CV schemes (naive, block, buffered block, walk-forward). The dashed vertical line marks the train/test split; grey points denote the observed test values. Panel titles report the selected hyperparameter and the resulting test MSE}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {S\&P 500} dataset: train (blue) and test-set predictions (red) under hyperparameters selected by each CV scheme. Rows correspond to regression methods (kernel regression, penalized splines, XGBoost) and columns correspond to CV schemes (naive, block, buffered block, walk-forward). The dashed vertical line marks the train/test split; grey points denote observed test values. Panel titles report the selected hyperparameter and the resulting test MSE.}}{14}{figure.13}\protected@file@percent }
\newlabel{fig:sp500_train_test_preds}{{13}{14}{\textbf {S\&P 500} dataset: train (blue) and test-set predictions (red) under hyperparameters selected by each CV scheme. Rows correspond to regression methods (kernel regression, penalized splines, XGBoost) and columns correspond to CV schemes (naive, block, buffered block, walk-forward). The dashed vertical line marks the train/test split; grey points denote observed test values. Panel titles report the selected hyperparameter and the resulting test MSE}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Median test-set error on synthetic data across experimental conditions. Each panel corresponds to a signal type (Local Bump, Smooth Trend, Nonlinear). Rows indicate the error process (Seasonal, ARIMA, MA(5), AR(1)), and within each row the three groups correspond to regression methods (Kernel, Spline, XGBoost). For each method, the four annotated cells report the median test MSE obtained after selecting the hyperparameter using the indicated CV scheme (Naive randomized $K$-fold, Block $K$-fold, Buffered block, Walk-forward) and refitting on the full training series. Cell color encodes the median test MSE (darker indicates larger error; color scale shown at right).}}{15}{figure.14}\protected@file@percent }
\newlabel{fig:median_test_mse_heatmap}{{14}{15}{Median test-set error on synthetic data across experimental conditions. Each panel corresponds to a signal type (Local Bump, Smooth Trend, Nonlinear). Rows indicate the error process (Seasonal, ARIMA, MA(5), AR(1)), and within each row the three groups correspond to regression methods (Kernel, Spline, XGBoost). For each method, the four annotated cells report the median test MSE obtained after selecting the hyperparameter using the indicated CV scheme (Naive randomized $K$-fold, Block $K$-fold, Buffered block, Walk-forward) and refitting on the full training series. Cell color encodes the median test MSE (darker indicates larger error; color scale shown at right)}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{How the Findings Align with Theory}{15}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Practical Recommendations}{16}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{16}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Limitations and Future Work}{16}{section*.45}\protected@file@percent }
\abx@aux@page{10}{16}
\abx@aux@page{11}{16}
\abx@aux@page{12}{16}
\abx@aux@page{13}{16}
\abx@aux@page{14}{16}
\abx@aux@page{15}{16}
\abx@aux@page{16}{16}
\@writefile{toc}{\contentsline {section}{\numberline {A}Real Data Diagnostics}{17}{appendix.A}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Air Quality (UCI): time series plot (left), ACF (center), and PACF (right) for the response series used in the real-data study. The ACF shows strong short-lag dependence and pronounced repeating peaks at regular lags, indicating seasonal/periodic structure; the PACF shows dominant low-order partial correlations consistent with an autoregressive component in addition to seasonality. Shaded bands indicate approximate 95\% confidence intervals.}}{17}{figure.15}\protected@file@percent }
\newlabel{fig:air_quality_diagnostics}{{15}{17}{Air Quality (UCI): time series plot (left), ACF (center), and PACF (right) for the response series used in the real-data study. The ACF shows strong short-lag dependence and pronounced repeating peaks at regular lags, indicating seasonal/periodic structure; the PACF shows dominant low-order partial correlations consistent with an autoregressive component in addition to seasonality. Shaded bands indicate approximate 95\% confidence intervals}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces S\&P 500: close prices (top row) and first differences (bottom row), with corresponding ACF (center) and PACF (right). The close-price series exhibits extremely persistent autocorrelation consistent with nonstationary levels, while the differenced series shows near-zero autocorrelation beyond lag 0, consistent with a stationary increment/return-like process. Shaded bands indicate approximate 95\% confidence intervals.}}{17}{figure.16}\protected@file@percent }
\newlabel{fig:sp500_diagnostics}{{16}{17}{S\&P 500: close prices (top row) and first differences (bottom row), with corresponding ACF (center) and PACF (right). The close-price series exhibits extremely persistent autocorrelation consistent with nonstationary levels, while the differenced series shows near-zero autocorrelation beyond lag 0, consistent with a stationary increment/return-like process. Shaded bands indicate approximate 95\% confidence intervals}{figure.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Reproducibility and Implementation Details}{17}{appendix.B}\protected@file@percent }
\newlabel{app:repro}{{B}{17}{Reproducibility and Implementation Details}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{Data Generating Process and Experimental Design}{17}{section*.47}\protected@file@percent }
\newlabel{app:dgp}{{B}{17}{Data Generating Process and Experimental Design}{section*.47}{}}
\@writefile{toc}{\contentsline {subparagraph}{Deterministic Signal Specifications}{17}{section*.48}\protected@file@percent }
\newlabel{app:repro_errors}{{B}{18}{Error processes and parameters (\texttt {utils/errors.py})}{section*.49}{}}
\@writefile{toc}{\contentsline {subparagraph}{Error processes and parameters (\texttt  {utils/errors.py})}{18}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ \textbf  {Initialization and burn-in.}}{18}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ \textbf  {Processes used in the main experiments.}}{18}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Monte Carlo Design and Evaluation Protocol}{18}{section*.52}\protected@file@percent }
\newlabel{app:dgp_protocol}{{B}{18}{Monte Carlo Design and Evaluation Protocol}{section*.52}{}}
\@writefile{toc}{\contentsline {paragraph}{Replication Loop and Independence.}{18}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Cross-validation implementations (\texttt  {utils/cv.py})}{18}{section*.54}\protected@file@percent }
\newlabel{app:repro_cv}{{B}{18}{Cross-validation implementations (\texttt {utils/cv.py})}{section*.54}{}}
\@writefile{toc}{\contentsline {subsection}{Models, tuning grids, and fixed settings}{19}{section*.55}\protected@file@percent }
\newlabel{app:repro_models}{{B}{19}{Models, tuning grids, and fixed settings}{section*.55}{}}
\@writefile{toc}{\contentsline {subsection}{Notes on timing benchmarks}{19}{section*.56}\protected@file@percent }
\newlabel{app:repro_timing}{{B}{19}{Notes on timing benchmarks}{section*.56}{}}
\abx@aux@read@bbl@mdfivesum{99A1C7C25066C10AE735A1CD81DCB513}
\abx@aux@defaultrefcontext{0}{arlot2010survey}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{BurmanChowNolan1994}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ChenGuestrin2016}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{EilersMarx1996}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Nadaraya1964}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Racine2000}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wood2024neighbourhood}{apa/global//global/global/global}
\gdef \@abspage@last{19}
